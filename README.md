# Backpropagation Neural Network ðŸ§ âš¡  
*Turning errors into learning â€” one gradient at a time.*


## ðŸ“Œ Overview
This project implements a **Neural Network from scratch** using the **Backpropagation algorithm**.  
The model learns by performing a forward pass to make predictions and a backward pass to minimize error using gradient descent.

Backpropagation is the mathematical engine that powers modern deep learning.


## ðŸŽ¯ Objectives
- Implement forward propagation
- Compute loss
- Perform backpropagation using the chain rule
- Update weights using gradient descent
- Train the network to learn patterns from data


## ðŸ§  Concepts Covered
- Feedforward Neural Networks
- Backpropagation
- Gradient Descent
- Activation Functions (Sigmoid / ReLU / Tanh)
- Loss Functions (MSE / Cross-Entropy)
- Chain Rule in Calculus


## ðŸ”¬ Algorithm Flow

Step 1: Initialize weights randomly  
Step 2: Forward pass  
Step 3: Compute loss  
Step 4: Backward pass (calculate gradients)  
Step 5: Update weights  
Step 6: Repeat until convergence  

Mathematically:

w = w - Î· * (âˆ‚L/âˆ‚w)

Where:
- Î· = learning rate
- L = loss function
